{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install libraries**"
      ],
      "metadata": {
        "id": "73zDwtM_Ow2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spijkervet/SimCLR.git\n",
        "%cd SimCLR\n",
        "!mkdir -p logs && cd logs && wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar && cd ../\n",
        "!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
        "!pip install  pyyaml --upgrade"
      ],
      "metadata": {
        "id": "hdOvijQeOz5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install staintools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYZXZ__v2wjb",
        "outputId": "1ed746dc-0c2c-4198-d297-32d32420c5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting staintools\n",
            "  Downloading staintools-2.1.2.tar.gz (7.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from staintools) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from staintools) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from staintools) (4.7.0.72)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->staintools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->staintools) (1.16.0)\n",
            "Building wheels for collected packages: staintools\n",
            "  Building wheel for staintools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for staintools: filename=staintools-2.1.2-py3-none-any.whl size=14048 sha256=3e8fa2cce70324cdeab026ef8f002f3afa51bd93660381b8b1b6bbaed56df828\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/01/30/d5b79f92442193b7e3229cb20fb8458e765594cc2251f87ae6\n",
            "Successfully built staintools\n",
            "Installing collected packages: staintools\n",
            "Successfully installed staintools-2.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LvnKgs30up",
        "outputId": "9603bb89-fe9c-44ac-8f30-9445c24058cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spams\n",
            "  Downloading spams-2.6.5.4.tar.gz (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/2.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from spams) (1.22.4)\n",
            "Requirement already satisfied: Pillow>=6.0 in /usr/local/lib/python3.10/dist-packages (from spams) (8.4.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from spams) (1.10.1)\n",
            "Building wheels for collected packages: spams\n",
            "  Building wheel for spams (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spams: filename=spams-2.6.5.4-cp310-cp310-linux_x86_64.whl size=4490019 sha256=3db04adaceb4b06936e97371fe0232afbf6b22ad86fa0e3f0210f52ec8fe4e5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/9d/1f/60a6c60785b07ad4f087297f04b723e665a97afcd5112ffb06\n",
            "Successfully built spams\n",
            "Installing collected packages: spams\n",
            "Successfully installed spams-2.6.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoiBXDi15-ao",
        "outputId": "9b914423-5b9e-4388-e45e-e77bf8a8482b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "36JGgPC66A7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from staintools import read_image, StainNormalizer"
      ],
      "metadata": {
        "id": "cMdfTEuh4hCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A test with stain normalizer**"
      ],
      "metadata": {
        "id": "uCk2QRPJO7pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = StainNormalizer(method='vahadane')\n",
        "source_image = read_image('/content/assd.jpg')\n",
        "target_image = read_image('/content/images.jpg')\n",
        "normalizer.fit(target_image)\n",
        "normalized_image = normalizer.transform(source_image)"
      ],
      "metadata": {
        "id": "bmebozwj4pGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imwrite('normalized_image.jpg', normalized_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZJDdYRe6GxN",
        "outputId": "b79fdd12-038e-448b-e4a9-078c76edc58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnQMVpnJ96fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get data**"
      ],
      "metadata": {
        "id": "adACoRZHPQ21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "diEueRLDt_Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1"
      ],
      "metadata": {
        "id": "6uuugYlBPWJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b762c8e3-36f3-4c3d-bd5e-5548b65346e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-15 08:20:40--  https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_test_x.h5.gz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 800875929 (764M) [application/octet-stream]\n",
            "Saving to: ‘camelyonpatch_level_2_split_test_x.h5.gz?download=1’\n",
            "\n",
            "camelyonpatch_level 100%[===================>] 763.77M  12.3MB/s    in 6m 11s  \n",
            "\n",
            "2023-07-15 08:26:52 (2.06 MB/s) - ‘camelyonpatch_level_2_split_test_x.h5.gz?download=1’ saved [800875929/800875929]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/drive/MyDrive/stain/camelyonpatch_level_2_split_test_x.h5.gz"
      ],
      "metadata": {
        "id": "iLp0xcGMPoea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_test_y.h5?download=1"
      ],
      "metadata": {
        "id": "QQcBV_kYPjuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py"
      ],
      "metadata": {
        "id": "MOYNUbMIL8sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = h5py.File('/content/drive/MyDrive/stain/camelyonpatch_level_2_split_test_y.h5', 'r')"
      ],
      "metadata": {
        "id": "9PD2zFnRPt3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = h5py.File('/content/drive/MyDrive/stain/camelyonpatch_level_2_split_test_x.h5', 'r')"
      ],
      "metadata": {
        "id": "0nrI4Ek4Pu8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = np.array(test_x.get('x'))\n",
        "test_y = np.array(test_y.get('y'))"
      ],
      "metadata": {
        "id": "j5hylSlWP3Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y = test_y.squeeze()"
      ],
      "metadata": {
        "id": "G8bhIhVeP5rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y.shape"
      ],
      "metadata": {
        "id": "Uw8e9y1wQAxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086f7162-bb17-4b29-f914-2c0f780dd54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32768,)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_x.shape"
      ],
      "metadata": {
        "id": "5yyPylNgQCra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca13f54d-57fe-4c97-846d-887634c18066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32768, 96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4Mje5KIQE3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1"
      ],
      "metadata": {
        "id": "A0k9kP8fQFPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30521e33-2b1b-4377-b215-78a47e16dd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-17 07:41:33--  https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_train_x.h5.gz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6421353462 (6.0G) [application/octet-stream]\n",
            "Saving to: ‘camelyonpatch_level_2_split_train_x.h5.gz?download=1’\n",
            "\n",
            "camelyonpatch_level 100%[===================>]   5.98G  4.32MB/s    in 55m 31s \n",
            "\n",
            "2023-07-17 08:37:06 (1.84 MB/s) - ‘camelyonpatch_level_2_split_train_x.h5.gz?download=1’ saved [6421353462/6421353462]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "file_name = \"/content/camelyonpatch_level_2_split_train_x.h5.gz\"\n",
        "\n",
        "hasher = hashlib.md5()\n",
        "\n",
        "with open(file_name, 'rb') as f:\n",
        "    buf = f.read()\n",
        "    hasher.update(buf)\n",
        "\n",
        "checksum = hasher.hexdigest()\n",
        "print(\"Checksum:\", checksum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS2bd-owISK9",
        "outputId": "d0da7f93-3677-4bab-cb89-30519504f650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checksum: 1571f514728f59376b705fc836ff4b63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_x.h5.gz"
      ],
      "metadata": {
        "id": "-N6louxlQZ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JjGKzlRQlnH",
        "outputId": "785d861e-fdb6-4d0b-9a34-2cedd581fb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-15 09:05:24--  https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_train_y.h5.gz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21378 (21K) [application/octet-stream]\n",
            "Saving to: ‘camelyonpatch_level_2_split_train_y.h5.gz?download=1’\n",
            "\n",
            "camelyonpatch_level 100%[===================>]  20.88K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-15 09:05:25 (281 MB/s) - ‘camelyonpatch_level_2_split_train_y.h5.gz?download=1’ saved [21378/21378]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_y.h5.gz"
      ],
      "metadata": {
        "id": "-RlJyMbYQnqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = h5py.File('/content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_x.h5', 'r')"
      ],
      "metadata": {
        "id": "TCyDY-SEQtNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_path = '/content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_x.h5'\n",
        "print(os.path.exists(file_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfmreDA-y3QO",
        "outputId": "71de9929-79b9-4ed6-db1a-34a74ee1d850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "file_path = '/content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_x.h5'\n",
        "chunk_size = 1000  # Define the size of each chunk\n",
        "\n",
        "with h5py.File(file_path, 'r') as file:\n",
        "    dataset = file['dataset_name']  # Replace 'dataset_name' with the actual dataset name in the HDF5 file\n",
        "    total_samples = dataset.shape[0]\n",
        "\n",
        "    # Create an empty list to store the chunks\n",
        "    chunks = []\n",
        "\n",
        "    for start in range(0, total_samples, chunk_size):\n",
        "        end = min(start + chunk_size, total_samples)\n",
        "        data_chunk = dataset[start:end]  # Read a chunk of data\n",
        "\n",
        "        # Append the chunk to the list\n",
        "        chunks.append(data_chunk)\n",
        "\n",
        "    # Concatenate all the chunks into a single array\n",
        "    x_data = np.concatenate(chunks, axis=0)\n",
        "\n",
        "    # Print the shape of the final data array\n",
        "    print(f\"x_data shape: {x_data.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "mtXo1XHxyU2W",
        "outputId": "f6925b34-6f4e-4f3e-c22c-6582d7d61bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-01e1df4bc951>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchunk_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m  \u001b[0;31m# Define the size of each chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_name'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace 'dataset_name' with the actual dataset name in the HDF5 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_x.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = h5py.File('/content/drive/MyDrive/stain/camelyonpatch_level_2_split_train_y.h5', 'r')"
      ],
      "metadata": {
        "id": "0YQaXVyAQ4hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.array(train_x.get('x'))\n",
        "train_y = np.array(train_y.get('y'))"
      ],
      "metadata": {
        "id": "Ax6oA9drROcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train_y.squeeze()"
      ],
      "metadata": {
        "id": "bHQeVPBTRSuK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "4d84ca09-6444-4b98-8969-64a0f4381282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ac6e628c3d00>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'File' object has no attribute 'squeeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "id": "vtK2MnxoRVZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1kkWyEMRY5H",
        "outputId": "9484749d-7116-43af-a66c-0f786b48d12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32768,)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWic34qbSNMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_valid_x.h5.gz"
      ],
      "metadata": {
        "id": "hgnxDWGJSNXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/1494286/files/camelyonpatch_level_2_split_valid_y.h5"
      ],
      "metadata": {
        "id": "LtfHL9-_EDyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d /content/camelyonpatch_level_2_split_valid_x.h5.gz"
      ],
      "metadata": {
        "id": "oLNffpajERFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_x = h5py.File('/content/camelyonpatch_level_2_split_valid_x.h5', 'r')"
      ],
      "metadata": {
        "id": "Qyg081qTEVHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_y = h5py.File('/content/camelyonpatch_level_2_split_valid_y.h5', 'r')"
      ],
      "metadata": {
        "id": "_Y-2Wp-MEVKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_x = np.array(valid_x.get('x'))\n",
        "valid_y = np.array(valid_y.get('y'))"
      ],
      "metadata": {
        "id": "Y7zdzjU_EsfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_y = valid_y.squeeze()"
      ],
      "metadata": {
        "id": "I8V3_UpREusR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_x.shape"
      ],
      "metadata": {
        "id": "b5Os54t9E0TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_y.shape"
      ],
      "metadata": {
        "id": "MwC9RYaiE5Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "O8EPoVLiPGin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import PCAM\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import h5py\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "uBbwUVhf96jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simclr.modules import NT_Xent"
      ],
      "metadata": {
        "id": "nNBftRwCA6Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform = transforms.Compose([\n",
        "#    transforms.RandomHorizontalFlip(),\n",
        "#    transforms.RandomVerticalFlip(),\n",
        "#    transforms.RandomRotation(20),\n",
        "#    transforms.RandomResizedCrop(size=224, scale=(0.1, 0.9)),\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Resize((896, 896)),\n",
        "#    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.08),\n",
        "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "#])"
      ],
      "metadata": {
        "id": "B49GZ14397j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augmented_train_dataset = PCAM(root='.', split='train', transform=transform, download=True)\n",
        "#augmented_valid_dataset = PCAM(root='.', split='valid', transform=transform, download=True)\n",
        "#augmented_test_dataset = PCAM(root='.', split='test', transform=transform, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO2u-YRW-f0_",
        "outputId": "550cd2df-496d-4461-f519-6aaa7bd3e392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: pcam/camelyonpatch_level_2_split_train_x.h5.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128"
      ],
      "metadata": {
        "id": "Niw9F3L8AYiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_loader = torch.utils.data.DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "#valid_loader = torch.utils.data.DataLoader(augmented_valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "#test_loader = torch.utils.data.DataLoader(augmented_test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "_cRIk-oI_zdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CustomDataset(data.Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "# Define any additional transformations you want to apply\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts the image to a PyTorch tensor\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "sX6FTmPZScwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of custom dataset\n",
        "train_dataset = CustomDataset(train_x, train_y, transform=transform)\n",
        "\n",
        "# Access a sample image from the dataset\n",
        "sample_image = train_dataset[0]\n",
        "print(sample_image.shape)"
      ],
      "metadata": {
        "id": "qtltWlQBStCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of custom dataset\n",
        "test_dataset = CustomDataset(test_x, test_y, transform=transform)\n",
        "\n",
        "# Access a sample image from the dataset\n",
        "sample_image = test_dataset[0]\n",
        "print(sample_image.shape)"
      ],
      "metadata": {
        "id": "et1dWO86TQ1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of custom dataset\n",
        "val_dataset = CustomDataset(valid_x, valid_y, transform=transform)\n",
        "\n",
        "# Access a sample image from the dataset\n",
        "sample_image = val_dataset[0]\n",
        "print(sample_image.shape)"
      ],
      "metadata": {
        "id": "G6yAegPYTXHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
        "val_loader = data.DataLoader(val_dataset, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "d9acjT6DQlGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderModel(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.leaky_relu(self.fc1(x))\n",
        "        x = self.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lBHx3ny7Rbdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the network architecture\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.Decoder = DecoderModel(256, 2)\n",
        "\n",
        "        # Encoder Backbone (ResNet18)\n",
        "        self.encoder = models.resnet18(pretrained=False)\n",
        "        self.encoder.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.encoder.layer1[0].conv1 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.encoder.layer2[0].conv1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.encoder.layer3[0].conv1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.encoder.layer4[0].conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.leaky_relu = nn.LeakyReLU(inplace=True)\n",
        "\n",
        "    def forward(self,  x_i, x_j):\n",
        "        # Encoder Backbone\n",
        "        x_i = self.encoder(x_i)\n",
        "        # Flatten the feature maps\n",
        "        x_i = torch.flatten(x_i, 1)\n",
        "        # Fully Connected Layers with Leaky ReLU activation\n",
        "        z_i = self.leaky_relu(self.fc1(x_i))\n",
        "\n",
        "        x_j = self.encoder(x_j)\n",
        "        x_j = torch.flatten(x_j, 1)\n",
        "        z_j = self.leaky_relu(self.fc1(x_j))\n",
        "\n",
        "        y_i = self.Decoder(z_i)\n",
        "        y_j = self.Decoder(z_j)\n",
        "\n",
        "\n",
        "        return z_i, z_j, y_i, y_j\n"
      ],
      "metadata": {
        "id": "jJsKbU51GOKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XaISZ5I1HWPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()"
      ],
      "metadata": {
        "id": "1x5831gDKeSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "iJXoejoxKA-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "vxbPO-OiKNiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.7\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "swBzlrsV0No6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_decoder = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = .0001)\n",
        "criterion_contrastive = NT_Xent(batch_size, temperature, world_size=1)"
      ],
      "metadata": {
        "id": "h8idsc-uKIwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "def augment(x):\n",
        "    # apply random resized crop and color jitter\n",
        "    p1 = torch.bernoulli(torch.tensor(0.5))\n",
        "    if(p1 == 1):\n",
        "      x = transforms.RandomResizedCrop(size=224, scale=(0.1, 0.9))(x)\n",
        "\n",
        "    # apply random resized crop and color jitter\n",
        "    p1 = torch.bernoulli(torch.tensor(0.5))\n",
        "    if(p1 == 1):\n",
        "      x = transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.08)(x)\n",
        "\n",
        "    # apply random horizontal flip and random rotation\n",
        "    p1 = torch.bernoulli(torch.tensor(0.5))\n",
        "    if(p1 == 1):\n",
        "      x = transforms.RandomHorizontalFlip()(x)\n",
        "\n",
        "    # apply random horizontal flip and random rotation\n",
        "    p1 = torch.bernoulli(torch.tensor(0.5))\n",
        "    if(p1 == 1):\n",
        "      x = transforms.RandomRotation(20)(x)\n",
        "\n",
        "    # apply random horizontal flip and random rotation\n",
        "    p1 = torch.bernoulli(torch.tensor(0.5))\n",
        "    if(p1 == 1):\n",
        "      x = transforms.RandomVerticalFlip(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Ldr_L0kAzKvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods = ['reinhard', 'macenko', 'vahadane']"
      ],
      "metadata": {
        "id": "Ms0rS5jF96fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):#tia toolbox\n",
        "    # iterate over the data loader\n",
        "    for x_i, labels in train_loader:\n",
        "        # generate a random index\n",
        "        index = torch.randint(0, batch_size, (1,))\n",
        "        # select the image and label at that index\n",
        "        x = x_i[index]\n",
        "\n",
        "        normalizer = StainNormalizer(method= random.choice(methods))\n",
        "        normalizer.fit(target_image)\n",
        "        normalized_image = normalizer.transform(source_image)\n",
        "\n",
        "        y = labels[index]\n",
        "        x_i_aug = augment(x_i)\n",
        "\n",
        "        # encode and decode the two augmented views of the same image\n",
        "        z_i, z_j, y_i, y_j = model(x_i, x_i_aug)\n",
        "        # z_x, ... = model(x)\n",
        "        # z_x_aug, ... = model(x_aug)\n",
        "\n",
        "        # compute the contrastive loss and the decoder loss\n",
        "        loss_contrastive = criterion_contrastive(z_i, z_j)\n",
        "        loss_decoder_i = criterion_decoder(y_i, labels)\n",
        "        loss_decoder_j = criterion_decoder(y_j, labels)\n",
        "        loss_decoder = (loss_decoder_i + loss_decoder_j) / 2\n",
        "\n",
        "        # combine the losses with some weights (you can tune these)\n",
        "        alpha = 0.5 # weight for contrastive loss\n",
        "        beta = 0.5 # weight for decoder loss\n",
        "        loss = alpha * loss_contrastive + beta * loss_decoder\n",
        "\n",
        "        # update the model and decoder parameters\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_dataset)\n",
        "        val_acc = val_correct / len(val_dataset)\n",
        "\n",
        "    wandb.log({\"epoch\": epoch + 1,\n",
        "               \"train_loss\": train_loss,\n",
        "               \"val_loss\": val_loss,\n",
        "               \"val_accuracy\": val_acc})\n",
        "\n",
        "    # print the epoch and the losses\n",
        "    print(f'Epoch {epoch}, Loss_contrastive: {loss_contrastive.item()}, Loss_decoder: {loss_decoder.item()}')"
      ],
      "metadata": {
        "id": "BHpOrP7VzUnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_acc = test_correct / len(test_dataset)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "oQ_YGbB6KjkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amhlo3XnKjnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfDkc0qsKjp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYmFqo6CKjtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}